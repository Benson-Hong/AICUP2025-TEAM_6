# -*- coding: utf-8 -*-
"""「競賽任務2程式碼」git上傳

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JP3G5jly4hTfLWVrlADfmnPr-yb6ZbSk
"""

from huggingface_hub import login
# 貼上您剛剛複製的 Hugging Face token YOUR_HUGGING_FACE_TOKEN
# 系統會提示您輸入 token，或您可以直接將 token 放在引號內
login(token=" YOUR_HUGGING_FACE_TOKEN") # 將 YOUR_HUGGING_FACE_TOKEN 替換為您複製的 token
print("Hugging Face 登入成功！")

# --- 步驟 1: 掛載 Google Drive ---
# 這是為了讓 Colab 能夠存取您雲端硬碟中的檔案
from google.colab import drive
print("正在掛載 Google Drive...")
drive.mount('/content/drive')
print("Google Drive 掛載完成。")

# --- 2025/06/07 新增: Hugging Face 登入區塊 (針對部分受限模型) ---
from huggingface_hub import login
# Qwen 模型可能需要 Hugging Face Token。
# 為了避免在公開筆記本中暴露 token，您可以運行一次 `from huggingface_hub import notebook_login; notebook_login()`
# 或者從 Colab 的左側面板選擇「Secrets」(🔑) 標籤，添加一個名為 `HF_TOKEN` 的 secret，並將您的 token 值填入其中。
try:
    login() # 嘗試從環境變數或交互式登入
    print("Hugging Face 登入成功！")
except Exception as e:
    print(f"Hugging Face 登入失敗: {e}. 部分模型可能無法載入。請確認您的 token 已設置。")

# --- 步驟 2: 測試 PyTorch 與 CUDA 環境 ---
import torch
print("\n--- PyTorch 環境檢查 ---")
print(f"PyTorch 版本: {torch.__version__}")
print(f"CUDA 是否可用: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA 設備名稱: {torch.cuda.get_device_name(0)}")
else:
    print("警告: CUDA 不可用，模型將在 CPU 上運行，速度會較慢。")

# --- 步驟 3: 安裝必要的函式庫 (強制安裝最新開發版 transformers 和 bitsandbytes) ---
print("\n--- 安裝必要的函式庫 ---")
# transformers, bitsandbytes, accelerate 是 LLM 量化載入所必需的
# datasets 和 openai-whisper 用於資料處理 and ASR
# *** 關鍵修改：確保 bitsandbytes 和 accelerate 也被安裝，以及最新 transformers ***
!pip install -q bitsandbytes accelerate
!pip install -q git+https://github.com/huggingface/huggingface_hub # 確保huggingface_hub是最新的
!pip install -q git+https://github.com/huggingface/transformers.git # 嘗試最新開發版
!pip install -q datasets openai-whisper
print("函式庫安裝完成。")

# --- 步驟 4: 載入 LLM 模型 Qwen/Qwen1.5-7B-Chat (8-bit 量化) ---
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# --- 更換為 Qwen/Qwen1.5-7B-Chat 模型 ---
model_name = 'Qwen/Qwen1.5-7B-Chat' # <--- 關鍵修改：模型名稱已修正！

print(f"\n--- 正在載入 LLM 模型: {model_name} (8-bit 量化) ---")

tokenizer = AutoTokenizer.from_pretrained(model_name)

# --- 新增: 處理 tokenizer 的 pad_token ---
# Qwen 通常有 pad_token_id
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token # Qwen 的 EOS token 通常為 <|endoftext|> 或 <|im_end|>
    print(f"Tokenizer 的 pad_token 已設置為 eos_token: {tokenizer.eos_token}")

# 使用 BitsAndBytesConfig 進行 8-bit 量化載入
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True, # 載入 8-bit 量化模型
    # bnb_8bit_compute_dtype=torch.float16, # 8-bit 默認使用 float16 進行計算，可省略
)

# 使用 from_pretrained 載入模型 (移除 pipeline 相關邏輯)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config, # 使用量化配置參數
    torch_dtype=torch.float16, # 8-bit 量化通常配合 float16
    device_map="auto", # 自動分配設備
    trust_remote_code=True # Qwen 模型可能需要設置為 True
)

# 確保模型的 pad_token_id 也被設置 (如果模型 config 中沒有，則從 tokenizer 獲取)
if model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.pad_token_id
    print(f"模型配置的 pad_token_id 已設置為 Tokenizer 的 pad_token_id: {tokenizer.pad_token_id}")

print(f"LLM 模型 {model_name} 載入完成。")

# --- 步驟 5: 載入 Whisper 模型 (更換為 'small' 提高準確度) ---
import whisper
import torch

print("\n--- 正在載入 Whisper 模型 ---")
# 根據您的需求選擇 'base' 或 'small'。'small' 效果更好，但 'base' 更快。
# 如果記憶體足夠，推薦使用 'small' (通常 T4 GPU 足夠)
WHISPER_MODEL_SIZE = "small" # <--- 建議從 "tiny" 改為 "small" 以提高準確度

whisper_model_for_transcription = whisper.load_model(WHISPER_MODEL_SIZE, device="cuda" if torch.cuda.is_available() else "cpu")
print(f"Whisper 模型 ({WHISPER_MODEL_SIZE}) 載入完成。")
print(f"載入的 Whisper 模型類型: {type(whisper_model_for_transcription)}")

# --- 步驟 6: 解壓縮 Private Dataset 到 Colab 臨時目錄 (帶密碼) ---
import os
import zipfile
import shutil
from datasets import Audio, Dataset, Features, Value # 也在此處引入，確保所有必要函式庫在同一區塊

# 原始的 .zip 檔案在 Google Drive 上的路徑
google_drive_zip_path = "/content/drive/MyDrive/private dataset/Private dataset.zip"

# Colab 內部用於解壓縮的臨時目標資料夾
colab_temp_extracted_private_dir = "/content/private_dataset_unzipped/"

# *** 您的密碼已在此處設定！ ***
# 請注意：zipfile 模組要求密碼為 bytes 型態，所以前面有 b'
zip_password = b'aicup20660205'

print(f"\n--- 正在從 Google Drive 解壓縮私人資料集至 Colab 臨時目錄 (使用密碼) ---")
print(f"檢查壓縮檔: {google_drive_zip_path}")

# --- 新增的解壓縮檢查功能 ---
if os.path.exists(colab_temp_extracted_private_dir) and os.listdir(colab_temp_extracted_private_dir):
    print(f"目標目錄 '{colab_temp_extracted_private_dir}' 已存在且不為空，跳過解壓縮。")
    # 如果已經解壓縮，我們仍然需要確認 audio_dir 的路徑
    final_audio_dir_candidate = None
    for item in os.listdir(colab_temp_extracted_private_dir):
        full_path = os.path.join(colab_temp_extracted_private_dir, item)
        if os.path.isdir(full_path) and item.lower() == "private":
            final_audio_dir_candidate = full_path
            break
        elif os.path.isdir(full_path) and os.path.exists(os.path.join(full_path, "private")):
            final_audio_dir_candidate = os.path.join(full_path, "private")
            break

    if final_audio_dir_candidate and os.path.exists(final_audio_dir_candidate):
        private_dataset_audio_dir = final_audio_dir_candidate # <--- 設定這個變數
        print(f"**確認私人資料集音訊檔實際路徑為: {private_dataset_audio_dir}**")
    else:
        print("\n警告: 目錄已存在，但無法自動確認 'private' 音訊資料夾的最終路徑。")
        print("請根據上面列出的目錄內容，手動檢查 Colab 左側文件瀏覽器，找到正確的路徑並設定 'private_dataset_audio_dir'。")
        # 如果無法確認路徑，則暫時設為 None 或讓後續程式碼報錯
        private_dataset_audio_dir = None
else:
    # --- 原有的解壓縮邏輯 (只有在目標目錄不存在或為空時才執行) ---
    if not os.path.exists(google_drive_zip_path):
        print(f"錯誤: 壓縮檔 '{google_drive_zip_path}' 未找到。請檢查 Google Drive 路徑是否正確，或檔案是否存在。")
        raise FileNotFoundError(f"Zip file not found at {google_drive_zip_path}")
    else:
        print(f"找到壓縮檔，正在解壓縮至: {colab_temp_extracted_private_dir}")
        try:
            # 清理舊的臨時解壓縮目錄 (如果存在)，確保每次執行都是新的狀態
            if os.path.exists(colab_temp_extracted_private_dir):
                shutil.rmtree(colab_temp_extracted_private_dir)
                print(f"已清理舊的臨時目錄: {colab_temp_extracted_private_dir}")

            # 創建新的臨時解壓縮目錄
            os.makedirs(colab_temp_extracted_private_dir, exist_ok=True)

            with zipfile.ZipFile(google_drive_zip_path, 'r') as zip_ref:
                zip_ref.extractall(colab_temp_extracted_private_dir, pwd=zip_password)
            print("解壓縮完成！")

            print("\nColab 臨時解壓縮目錄內容 (供您確認結構):")
            final_audio_dir_candidate = None
            for item in os.listdir(colab_temp_extracted_private_dir):
                full_path = os.path.join(colab_temp_extracted_private_dir, item)
                print(f"- {item} (是資料夾: {os.path.isdir(full_path)})")
                if os.path.isdir(full_path) and item.lower() == "private":
                    final_audio_dir_candidate = full_path
                    break
                elif os.path.isdir(full_path) and os.path.exists(os.path.join(full_path, "private")):
                    final_audio_dir_candidate = os.path.join(full_path, "private")
                    break

            if final_audio_dir_candidate and os.path.exists(final_audio_dir_candidate):
                private_dataset_audio_dir = final_audio_dir_candidate # <--- 設定這個變數
                print(f"\n**確認私人資料集音訊檔實際路徑為: {private_dataset_audio_dir}**")
            else:
                print("\n警告: 無法自動確認 'private' 音訊資料夾的最終路徑。")
                print("請根據上面列出的目錄內容，手動檢查 Colab 左側文件瀏覽器，找到正確的路徑並設定 'private_dataset_audio_dir'。")
                private_dataset_audio_dir = None

        except zipfile.BadZipFile:
            print(f"錯誤: 壓縮檔 '{google_drive_zip_path}' 不是有效的 Zip 檔案。")
            raise
        except RuntimeError as e:
            print(f"解壓縮時發生錯誤: {e}")
            print("這通常表示密碼不正確，或者壓縮檔確實是加密的。")
            raise
        except Exception as e:
            print(f"解壓縮時發生其他錯誤: {e}")
            raise

# --- 路徑配置 ---
# 確保 private_dataset_audio_dir 已經被設定
if private_dataset_audio_dir is None:
    raise FileNotFoundError("私人資料集音訊目錄未正確設定。無法繼續。")

task1_private_answer_path = '/content/task1_answer.txt'
task2_private_output_dir = '/content/drive/MyDrive/AICUP_Results_Private_Dataset/'
task2_private_output_path = os.path.join(task2_private_output_dir, 'task2_private_answer.txt')

print("\n--- 路徑配置 ---")
print(f"私人資料集音訊目錄 (Colab 臨時): {private_dataset_audio_dir}")
print(f"私人資料集 Task 1 答案路徑 (Colab 臨時): {task1_private_answer_path}")
print(f"私人資料集 Task 2 輸出路徑 (Google Drive): {task2_private_output_path}")

# 確保輸出目錄存在
os.makedirs(task2_private_output_dir, exist_ok=True)

# --- 正在載入私人測試資料集音訊檔 ---
print("\n--- 正在載入私人測試資料集音訊檔 ---")
audio_files = []
if os.path.exists(private_dataset_audio_dir):
    for filename in sorted(os.listdir(private_dataset_audio_dir)):
        if filename.endswith(".wav") or filename.endswith(".flac"):
            audio_id = os.path.splitext(filename)[0]
            audio_files.append({"audio_id": audio_id, "audio": os.path.join(private_dataset_audio_dir, filename)})
else:
    print(f"錯誤: 私人資料集音訊目錄 '{private_dataset_audio_dir}' 不存在。請確認解壓縮步驟成功且路徑正確。")
    raise FileNotFoundError(f"Audio directory not found at {private_dataset_audio_dir}")

features = Features({
    "audio_id": Value("string"),
    "audio": Audio(sampling_rate=16000)
})
private_validation_dataset = Dataset.from_list(audio_files, features=features)
print(f"已載入 {len(private_validation_dataset)} 個私人測試資料集音訊檔案。")

# --- 正在從 Task 1 檔案 (/content/task1_answer.txt) 讀取轉錄文字 ---
transcriptions = {}
try:
    with open(task1_private_answer_path, "r", encoding="utf-8") as f:
        for line in f:
            if "\t" in line:
                audio_id, text = line.strip().split("\t", 1)
                transcriptions[audio_id] = text
    print(f"已從 Task 1 檔案 ({task1_private_answer_path}) 讀取 {len(transcriptions)} 個轉錄文字。")
except FileNotFoundError:
    print(f"錯誤: 私人 Task 1 輸出檔案 ({task1_private_answer_path}) 未找到。請確認您已將 'task1_answer.txt' 上傳至 Colab 的 /content/ 目錄，或先完成所有檔案的 Task 1 處理。")
    raise FileNotFoundError(f"Private Task 1 output file not found at {task1_private_answer_path}.")
except Exception as e:
    print(f"讀取私人 Task 1 輸出檔案時發生錯誤: {e}")
    transcriptions = {}
    raise # 重新拋出異常，讓問題更明顯

# --- 步驟 10: 從 LLM 的輸出中提取 JSON 內容的輔助函式 ---
# 這個函式用於從 LLM 的自由文本輸出中精確提取 JSON 結構
import re
import json
import time
import os

def extract_json_from_output(generated_text):
    """
    使用正則表達式從 LLM 的輸出中提取 JSON 內容。
    會優先尋找 ```json 區塊，如果沒有找到，會嘗試尋找 {} 包裹的內容作為備用。
    """
    json_match = re.search(r'```json\s*(.*?)\s*```', generated_text, re.DOTALL)
    if json_match:
        json_content = json_match.group(1).strip()
        if json_content:
            try:
                json.loads(json_content)
                print("     (使用主要方法成功提取並驗證 JSON)")
                return json_content
            except json.JSONDecodeError:
                print(f"     (主要方法提取到內容但 JSON 無效或不完整: {json_content[:200]}...)")
                # 嘗試清理不完整的 JSON，只保留最外層 {} 之間的內容
                clean_content = re.sub(r'(.*?)\s*```.*', r'\1', json_content, flags=re.DOTALL).strip()
                try:
                    json.loads(clean_content)
                    print(f"     (主要方法清理後成功提取並驗證 JSON)")
                    return clean_content
                except json.JSONDecodeError:
                    print(f"     (主要方法清理後 JSON 仍無效: {clean_content[:200]}...)")
                    pass

    all_json_candidates = []
    # 尋找所有可能被大括號 {} 包裹的內容 (嵌套的也考慮)
    # 這個正則表達式 (?R) 是遞歸匹配的關鍵，它會匹配嵌套的 {}
    for match_content in re.findall(r'\{(?:[^{}]|(?R))*\}+', generated_text, re.DOTALL):
        try:
            json.loads(match_content)
            all_json_candidates.append(match_content)
        except json.JSONDecodeError:
            pass

    if all_json_candidates:
        # 如果有多個 JSON 候選，選擇最長的一個
        fallback_content = max(all_json_candidates, key=len)
        print("     (使用備用方案提取到可能的 JSON)")
        return fallback_content

    print("     (未找到可提取的 JSON 內容)")
    return None

# --- 步驟 11: 開始處理私人資料集以生成 Task 2 提交檔案 (重新生成模式) ---
print("\n--- 正在處理私人資料集以生成 Task 2 提交檔案 (重新生成所有結果) ---")

# --- 主要處理邏輯 ---
if len(private_validation_dataset) == 0:
    print("錯誤: 私人測試資料集為空，無法進行 Task 2 處理。")
elif len(transcriptions) == 0:
    print("錯誤: 私人 Task 1 轉錄數據為空，無法進行 Task 2 處理。")
else:
    print(f"總共有 {len(private_validation_dataset)} 個檔案需要處理 Task 2。")
    print("將從頭開始處理所有檔案，結果將寫入新檔案。") # 這次選擇完全重新生成，不考慮斷點續傳

    if 'whisper_model_for_transcription' not in locals() or whisper_model_for_transcription is None:
        print("錯誤: 找不到 Whisper 模型變數 'whisper_model_for_transcription'。請確認載入模型的獨立儲存格已成功執行。無法進行 Task 2 處理。")
        print("請檢查並重新執行 Whisper 模型載入的程式碼區塊。")
    elif 'model' not in locals() or model is None or 'tokenizer' not in locals() or tokenizer is None:
        print("錯誤: 找不到 LLM 模型或 Tokenizer 變數。請確認載入 LLM 的儲存格已成功執行。無法進行 Task 2 推論。")
        print("請檢查並重新執行 LLM 模型載入的程式碼區塊。")
    else:
        print("正在建立按照 audio_id 排序的檔案列表用於處理...")
        try:
            sorted_audio_ids_with_indices = sorted(
                [(data['audio_id'], i) for i, data in enumerate(private_validation_dataset)],
                key=lambda item: item[0]
            )
            print(f"已建立包含 {len(sorted_audio_ids_with_indices)} 個檔案的排序列表。")
        except Exception as e:
            print(f"建立排序檔案列表時發生錯誤: {e}，無法按排序順序處理。")
            raise e

        # *** 新增的跳過檔案設定 ***
        # 設定您希望從第幾個檔案開始處理 (例如，99表示從索引99開始，即排序後的第100個檔案)
        START_INDEX = 99
        print(f"將跳過前 {START_INDEX} 個檔案，從排序列表中的第 {START_INDEX + 1} 個檔案開始處理。")

        # 以寫入模式 ('w') 打開 Task 2 輸出檔案，每次運行都創建新檔案
        with open(task2_private_output_path, "w", encoding="utf-8") as task2_file:
            start_time = time.time()
            processed_this_run = 0

            for sorted_i, (audio_id, original_i) in enumerate(sorted_audio_ids_with_indices):
                # *** 新增的跳過邏輯 ***
                if sorted_i < START_INDEX:
                    print(f"--- 跳過檔案 ID: {audio_id} (在排序列表中的位置: {sorted_i+1}/{len(private_validation_dataset)}) ---")
                    continue # 跳過當前檔案，繼續下一個

                data = private_validation_dataset[original_i]
                audio_path = data["audio"]["path"]
                transcription_text_from_task1 = transcriptions.get(audio_id)

                if transcription_text_from_task1 is None:
                    print(f"警告: 未在 Task 1 轉錄中找到檔案 ID: {audio_id} 的文字。跳過 Task 2 處理。")
                    continue

                print(f"\n--- 正在處理檔案 ID: {audio_id} (在排序列表中的位置: {sorted_i+1}/{len(private_validation_dataset)}) --- (Task 2)")

                try:
                    print("     正在重新運行 ASR 以獲取時間戳記...")
                    asr_result_for_timestamps = whisper_model_for_transcription.transcribe(
                        audio_path, fp16=False, word_timestamps=True
                    )
                    word_timestamps = asr_result_for_timestamps.get("segments", [])

                    print("     正在使用 LLM 進行敏感資訊辨識...")
                    transcription_text_for_llm = transcription_text_from_task1

                    # 這裡定義所有可能的敏感信息類別
                    all_shi_categories = [
                        "PATIENT", "DOCTOR", "USERNAME", "PERSONALNAME", "FAMILYNAME",
                        "PROFESSION", "ROOM", "DEPARTMENT", "HOSPITAL", "ORGANIZATION",
                        "STREET", "CITY", "STATE", "COUNTRY", "COUNTY", "ZIP", "LOCATION-OTHER",
                        "DISTRICT", "AGE", "DATE", "TIME", "DURATION", "SET", "PHONE", "FAX",
                        "EMAIL", "URL", "IPADDRESS", "OTHER", "SOCIAL_SECURITY_NUMBER",
                        "MEDICAL_RECORD_NUMBER", "HEALTH_PLAN_NUMBER", "ACCOUNT_NUMBER",
                        "LICENSE_NUMBER", "VEHICLE_ID", "DEVICE_ID", "BIOMETRIC_ID", "ID_NUMBER"
                    ]
                    # 構建 JSON 格式模板，包含所有類別
                    json_format_template = "{\n     \"text\": \"{transcription_text_for_llm}\","
                    for cat in all_shi_categories:
                        json_format_template += f"\n     \"{cat}\": [\"list all extracted {cat} here\"],"
                    json_format_template = json_format_template.rstrip(',') + "\n}" # 移除最後一個逗號，並結束 JSON

                    # --- LLM 提示詞 (Messages 格式，適合 Qwen 和其他 Chat 模型) ---
                    messages = [
                        {"role": "system", "content": "You are an expert in extracting sensitive health information. Your ONLY task is to return a valid JSON object. Do NOT include any other text, explanations, or code outside the JSON block. Ensure the JSON is well-formed and follows the specified schema. If a category has no corresponding information found in the text, its list in the JSON should be empty []."},
                        {"role": "user", "content": f'''Extract Sensitive Health Information (SHI) from the following text.
The text is a transcription of a doctor-patient conversation.

Identify and categorize the SHI into the following specific categories:
{', '.join(all_shi_categories)}.

Return ONLY the extracted SHI in a single JSON object. The JSON object MUST be enclosed within ```json and ``` blocks.
The JSON object must strictly follow this structure:
```json
{json_format_template}
Extract ONLY information that is explicitly mentioned in the text.

Here is the text: "{transcription_text_for_llm}"
'''
                        }
                    ]

                    # --- LLM 呼叫方式：使用 model.generate 重新啟用 KV Cache ---
                    # 應用聊天模板並分詞
                    chat_template_output = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)

                    input_ids = torch.tensor([chat_template_output]).to(model.device)
                    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=model.device)

                    output_tokens = model.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=1024, # 增加最大生成 token 數量以確保完整 JSON 輸出
                        do_sample=False, # 不進行採樣，輸出確定性較高
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        use_cache=True # <--- 關鍵修改：重新啟用 KV Cache，希望 Qwen 7B 兼容
                    )

                    # 解碼生成的 token
                    generated_text = tokenizer.decode(output_tokens[0][input_ids.shape[-1]:], skip_special_tokens=True)

                    print("     LLM Generated Response (partial):", generated_text[:500].replace('\n', ' ') + "..." if len(generated_text) > 500 else generated_text.replace('\n', ' '))
                    # --- LLM 呼叫方式結束 ---

                    json_content = extract_json_from_output(generated_text)

                    parsed_json = None
                    if not json_content:
                        print(f"     ❌ 無法從 LLM 回應中提取 JSON 內容: {audio_id}")
                    else:
                        try:
                            parsed_json = json.loads(json_content)
                            print("     成功解析 LLM 的 JSON 回應。")
                        except json.JSONDecodeError as e:
                            print(f"     ❌ JSON 解析錯誤 ({audio_id}), 錯誤: {e}")
                            parsed_json = None

                    shi_timestamps_found = []

                    if parsed_json is not None and word_timestamps:
                        # 將所有帶時間戳的詞語扁平化到一個列表中，方便遍歷
                        all_words_with_timestamps = []
                        for seg in word_timestamps:
                            if 'words' in seg:
                                all_words_with_timestamps.extend(seg["words"])

                        # 遍歷所有敏感信息類別
                        for category in all_shi_categories:
                            values = parsed_json.get(category, []) # 從解析的 JSON 中獲取該類別的值列表
                            if isinstance(values, list): # 確保 values 是列表
                                for value_text_from_llm in values: # 這是 LLM 提取的敏感詞文字
                                    if not value_text_from_llm or not isinstance(value_text_from_llm, str):
                                        continue # 跳過空的或非字串的值

                                    # 將 LLM 提取的敏感詞短語分解為單詞列表，並轉換為小寫，去除標點符號
                                    value_words_llm = re.findall(r'\b\w+\b', value_text_from_llm.lower())
                                    if not value_words_llm:
                                        continue # 如果分解後是空的，則跳過

                                    # 在 Whisper 轉錄的詞語列表中尋找匹配的連續詞語序列
                                    # 這裡需要一個嵌套循環來實現子序列匹配
                                    for i in range(len(all_words_with_timestamps) - len(value_words_llm) + 1):
                                        match_found = True
                                        current_match_words = [] # 儲存當前潛在匹配的 Whisper 詞語
                                        for j in range(len(value_words_llm)):
                                            asr_word_info = all_words_with_timestamps[i + j]
                                            # 將 Whisper 詞語也進行清理和轉換為小寫
                                            asr_word_lower = re.sub(r'[^\w]', '', asr_word_info['word']).lower()

                                            # 檢查是否匹配
                                            if asr_word_lower != value_words_llm[j]:
                                                match_found = False
                                                break # 不匹配，跳出內層循環
                                            current_match_words.append(asr_word_info)

                                        if match_found:
                                            # 如果找到完整匹配，提取時間戳和匹配文本
                                            start_time_match = current_match_words[0]['start']
                                            end_time_match = current_match_words[-1]['end']
                                            matched_text = " ".join([w['word'] for w in current_match_words]) # 使用原始 Whisper 詞語構建匹配文本

                                            shi_timestamps_found.append({
                                                "file_id": audio_id,
                                                "category": category,
                                                "start": start_time_match,
                                                "end": end_time_match,
                                                "text": matched_text
                                            })
                                            break # 找到一個匹配後，跳出當前敏感詞在所有 Whisper 詞語中的尋找，處理下一個敏感詞

                    # 輸出 Task 2 至 task2_private_output_path
                    if shi_timestamps_found: # 只在找到敏感詞並匹配到時間戳時才寫入
                        # 按照時間戳排序輸出，以便於評估
                        shi_timestamps_found.sort(key=lambda x: (x['start'], x['end']))
                        for item in shi_timestamps_found:
                            # 格式為: Filename{TAB}SHI_type{TAB}Start{TAB}End{TAB}Text
                            # Task 2 提交格式的 Time 是秒，精度要求
                            task2_file.write(f"{item['file_id']}\t{item['category']}\t{item['start']:.3f}\t{item['end']:.3f}\t{item['text']}\n")

                        # 為了防止程式中斷時數據丟失，及時將緩衝區的數據寫入磁盤
                        task2_file.flush()
                        os.fsync(task2_file.fileno())

                        print(f"     Task 2 結果已寫入，找到 {len(shi_timestamps_found)} 個帶時間戳的敏感詞。")
                        processed_this_run += 1
                    else:
                        print("     沒有找到帶時間戳的敏感詞，Task 2 沒有寫入。")

                except Exception as e:
                    print(f"     處理檔案 ID: {audio_id} 時發生未預期的錯誤 (Task 2): {e}")
                    # 如果處理單個檔案 Task 2 時出錯，記錄錯誤並繼續處理下一個檔案
                    pass # 使用 pass 跳過錯誤處理，繼續迴圈

            # --- Task 2 處理迴圈結束 ---
            # 在遍歷迴圈結束後，處理完成訊息
            end_time = time.time() # 結束計時
            elapsed_time = end_time - start_time

            # 重新計算 Task 2 已處理檔案總數 (因為本次運行是完全重新生成)
            try:
                with open(task2_private_output_path, "r", encoding="utf-8") as f:
                    # 統計文件中唯一的 audio_id 數量
                    final_processed_count_task2 = len(set(line.strip().split('\t', 1)[0] for line in f if line.strip()))
            except FileNotFoundError:
                final_processed_count_task2 = 0

            print(f"\n=== 私人測試資料集 Task 2 處理完成 (本次運行成功處理 {processed_this_run} 個檔案的 Task 2) ===")
            print(f"本次處理時間: {elapsed_time:.2f} 秒")
            print(f"Task 2 結果已保存到: {task2_private_output_path}，包含 {final_processed_count_task2} 個檔案的數據。")
            print("================================")

a